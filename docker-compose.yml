services:
  qdrant:
    image: qdrant/qdrant:v1.14.1
    container_name: qdrant
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API
    volumes:
      - ./qdrant_data:/qdrant/storage
    environment:
      # gRPC configuration
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__ENABLE_TLS=false
      # Performance optimizations
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=0  # Use all available cores
      - QDRANT__STORAGE__PERFORMANCE__MAX_OPTIMIZATION_THREADS=2
      # Optional: increase payload index threshold for better performance
      - QDRANT__STORAGE__OPTIMIZERS__PAYLOAD_INDEX_THRESHOLD=20000
    env_file:
      - .env.prod
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "timeout", "1", "bash", "-c", "</dev/tcp/localhost/6333"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    # Resource limits (adjust based on your system)
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    networks:
      - app_network

  app:
    build: .
    container_name: semantic_open_data_backend
    depends_on:
      qdrant:
        condition: service_healthy
#      embedder-baai-bge-m3:
#        condition: service_started
#      embedder-intfloat-multilingual-e5-base:
#        condition: service_started
#      embedder-jinaai-jina-embeddings-v3:
#        condition: service_started
#      embedder-sentence-transformers-labse:
#        condition: service_started
#      ollama:
#        condition: service_healthy
    env_file:
      - .env.prod
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      - models_cache:/app/models_cache
    restart: unless-stopped
    # Increase timeout for model loading
#    stop_grace_period: 60s
    networks:
      - app_network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 5s

  # Embedder services - different models for experiments
  embedder-baai-bge-m3:
    image: mskkote/embedder-baai-bge-m3:latest
    container_name: embedder-baai-bge-m3
    ports:
      - "8080:8080"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/healthz" ]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 5s
    networks:
      - app_network

  embedder-intfloat-multilingual-e5-base:
    image: mskkote/embedder-intfloat-multilingual-e5-base:latest
    container_name: embedder-intfloat-multilingual-e5-base
    ports:
      - "8081:8080"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/healthz" ]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 5s
    networks:
      - app_network

  embedder-jinaai-jina-embeddings-v3:
    image: mskkote/embedder-jinaai-jina-embeddings-v3:latest
    container_name: embedder-jinaai-jina-embeddings-v3
    ports:
      - "8082:8080"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/healthz" ]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 5s
    networks:
      - app_network

  embedder-sentence-transformers-labse:
    image: mskkote/embedder-sentence-transformers-labse:latest
    container_name: embedder-sentence-transformers-labse
    ports:
      - "8083:8080"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/healthz" ]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 5s
    networks:
      - app_network

  reranker:
    image: mskkote/reranker-nvidia-llama-nemotron-rerank-vl-1b-v2:latest
    container_name: reranker
    ports:
      - "8084:8000"
    environment:
      - MODEL_NAME=nvidia/llama-nemotron-rerank-vl-1b-v2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
#    deploy:
#      resources:
#        limits:
#          memory: 14G
    restart: unless-stopped
    networks:
      - app_network

  web:
    image: mskkote/open-data-ui:latest
    ports:
      - '80:80'
    environment:
      - VITE_API_URL=http://app:8000
    restart: unless-stopped
    networks:
      - app_network

  mongodb:
    image: mongo:7
    container_name: semantic_open_data_mongodb
    restart: unless-stopped
    env_file:
      - .env.prod
    ports:
      - '${MONGO_PORT:-27017}:27017'
    volumes:
      - mongodb_data:/data/db
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    networks:
      - app_network
    healthcheck:
      test: ['CMD', 'mongosh', '--eval', "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

#  ollama:
#      image: ollama/ollama:latest
#      container_name: ollama
#      ports:
#        - "11434:11434"
#      volumes:
#        - ollama_data:/root/.ollama
#        - ./init-ollama.sh:/init-ollama.sh:ro
#      environment:
#        - OLLAMA_HOST=0.0.0.0
#        - OLLAMA_MODELS=gemma3:4b
#      restart: unless-stopped
#      networks:
#        - app_network
#      healthcheck:
#        test: |
#          sh -c "ollama list >/dev/null 2>&1 && \
#                 ollama list 2>/dev/null | grep -q gpt-oss"
#        interval: 30s
#        timeout: 10s
#        retries: 10
#        start_period: 180s
#      deploy:
#        resources:
#          limits:
#            memory: 16G
#          reservations:
#            memory: 14G
#      entrypoint: ["/bin/sh", "/init-ollama.sh"]

networks:
  app_network:
    driver: bridge

volumes:
  models_cache:
  mongodb_data:
  ollama_data:
